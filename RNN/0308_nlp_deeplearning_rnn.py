# -*- coding: utf-8 -*-
"""0308_nlp_deeplearning_RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16FwIVj6DWfANdqaQsB7tVxjwrEEC2ukv
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Sequential, Model

import tensorflow_datasets as tfds

tf.random.set_seed(42)

imdb, info = tfds.load('imdb_reviews/subwords8k', with_info=True, as_supervised=True)

train_data, test_data = imdb['train'], imdb['test']

tokenizer = info.features['text'].encoder
print(tokenizer.subwords)

len(tokenizer.subwords)

sample_string = 'Tensorflow, from basics to mastery'

tokenized_string = tokenizer.encode(sample_string)
tokenized_string

for token in tokenized_string:
    print("{} -> {}".format(token, tokenizer.decode([token])))

original_string = tokenizer.decode(tokenized_string)
original_string

type(train_data)

BUFFER_SIZE = 10000
BATCH_SIZE = 64

train_dataset = train_data.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(train_dataset))
test_dataset = test_data.padded_batch(BATCH_SIZE, tf.compat.v1.data.get_output_shapes(test_data))

from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense

embedding_dim = 16
max_length = 120

model = Sequential()
model.add(Embedding(tokenizer.vocab_size, embedding_dim, input_length=max_length))
model.add(GlobalAveragePooling1D())
model.add(Dense(6, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

tokenizer.vocab_size

num_epochs = 10

hist = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)

import matplotlib.pyplot as plt

def plot_graphs(hist, string):
    plt.plot(hist.history[string])
    plt.plot(hist.history['val_'+string])
    plt.xlabel('Epochs')
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()

plot_graphs(hist, 'accuracy')
plot_graphs(hist, 'loss')

embedding_dim

from tensorflow.keras.layers import LSTM

model = Sequential([
                    Embedding(tokenizer.vocab_size, embedding_dim),
                    LSTM(64),
                    Dense(32, activation='relu'),
                    Dense(1, activation='sigmoid')
])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

from tensorflow.keras.layers import SimpleRNN, RNN
cell = layers.SimpleRNNCell(units=5, input_shape=(max_length, embedding_dim))
model = Sequential([
                    Embedding(tokenizer.vocab_size, embedding_dim),
                    SimpleRNN(64),
                    Dense(32, activation='relu'),
                    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

hist = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)

